{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized K-Means Clustering Tutorial\n",
    "\n",
    "This notebook demonstrates the massivedatascience-clusterer library for PySpark.\n",
    "\n",
    "## Features\n",
    "- Multiple Bregman divergences (Squared Euclidean, KL, Itakura-Saito, etc.)\n",
    "- Weighted clustering\n",
    "- Quality metrics (WCSS, BCSS, Calinski-Harabasz, Davies-Bouldin)\n",
    "- Model persistence\n",
    "- Full Spark ML Pipeline integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from massivedatascience.clusterer import GeneralizedKMeans\n",
    "\n",
    "# Create Spark session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"ClusteringTutorial\")\n",
    "    .config(\"spark.ui.enabled\", \"false\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Clustering with Squared Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with 3 clusters\n",
    "data = spark.createDataFrame([\n",
    "    (Vectors.dense([0.0, 0.0]),),\n",
    "    (Vectors.dense([0.5, 0.5]),),\n",
    "    (Vectors.dense([0.5, -0.5]),),\n",
    "    (Vectors.dense([5.0, 5.0]),),\n",
    "    (Vectors.dense([5.5, 5.0]),),\n",
    "    (Vectors.dense([5.0, 5.5]),),\n",
    "    (Vectors.dense([10.0, 0.0]),),\n",
    "    (Vectors.dense([10.5, 0.0]),),\n",
    "    (Vectors.dense([10.0, 0.5]),),\n",
    "], [\"features\"])\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train clustering model\n",
    "kmeans = GeneralizedKMeans(\n",
    "    k=3,\n",
    "    divergence=\"squaredEuclidean\",\n",
    "    maxIter=20,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "model = kmeans.fit(data)\n",
    "\n",
    "print(f\"Number of clusters: {model.numClusters}\")\n",
    "print(f\"\\nCluster centers:\")\n",
    "for i, center in enumerate(model.clusterCenters()):\n",
    "    print(f\"  Cluster {i}: {center}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = model.transform(data)\n",
    "predictions.select(\"features\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters\n",
    "predictions_pd = predictions.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster_id in range(model.numClusters):\n",
    "    cluster_data = predictions_pd[predictions_pd.prediction == cluster_id]\n",
    "    features = np.array(cluster_data.features.tolist())\n",
    "    plt.scatter(features[:, 0], features[:, 1], label=f\"Cluster {cluster_id}\", s=100)\n",
    "\n",
    "# Plot cluster centers\n",
    "centers = model.clusterCenters()\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', s=300, \n",
    "            edgecolors='black', linewidths=2, label='Centers')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('K-Means Clustering with Squared Euclidean Distance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Clustering Probability Distributions with KL Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create probability distribution data (document word distributions)\n",
    "prob_data = spark.createDataFrame([\n",
    "    # Technical documents\n",
    "    (Vectors.dense([0.5, 0.3, 0.1, 0.05, 0.05]),),\n",
    "    (Vectors.dense([0.6, 0.2, 0.1, 0.05, 0.05]),),\n",
    "    (Vectors.dense([0.55, 0.25, 0.1, 0.05, 0.05]),),\n",
    "    # Sports documents\n",
    "    (Vectors.dense([0.1, 0.1, 0.5, 0.2, 0.1]),),\n",
    "    (Vectors.dense([0.05, 0.15, 0.6, 0.15, 0.05]),),\n",
    "    # Food documents\n",
    "    (Vectors.dense([0.05, 0.1, 0.1, 0.35, 0.4]),),\n",
    "    (Vectors.dense([0.1, 0.05, 0.15, 0.3, 0.4]),),\n",
    "], [\"features\"])\n",
    "\n",
    "print(\"Probability distributions:\")\n",
    "prob_data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use KL divergence for probability distributions\n",
    "kmeans_kl = GeneralizedKMeans(\n",
    "    k=3,\n",
    "    divergence=\"kl\",\n",
    "    smoothing=1e-10,\n",
    "    maxIter=30,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "model_kl = kmeans_kl.fit(prob_data)\n",
    "\n",
    "print(f\"Number of clusters: {model_kl.numClusters}\")\n",
    "print(f\"\\nCluster centers (probability distributions):\")\n",
    "for i, center in enumerate(model_kl.clusterCenters()):\n",
    "    print(f\"  Cluster {i}: {center}\")\n",
    "    print(f\"    Sum: {np.sum(center):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions_kl = model_kl.transform(prob_data)\n",
    "predictions_kl.select(\"features\", \"prediction\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Quality Metrics and Finding Optimal K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different values of k\n",
    "data_cached = data.cache()\n",
    "\n",
    "results = []\n",
    "k_values = range(2, 7)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans_test = GeneralizedKMeans(k=k, maxIter=20, seed=42)\n",
    "    model_test = kmeans_test.fit(data_cached)\n",
    "    summary = model_test.summary\n",
    "    \n",
    "    results.append({\n",
    "        'k': k,\n",
    "        'wcss': summary.wcss,\n",
    "        'bcss': summary.bcss,\n",
    "        'ch_index': summary.calinskiHarabaszIndex,\n",
    "        'db_index': summary.daviesBouldinIndex\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize quality metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# WCSS (Elbow plot)\n",
    "axes[0, 0].plot(results_df['k'], results_df['wcss'], 'bo-', linewidth=2, markersize=8)\n",
    "axes[0, 0].set_xlabel('Number of Clusters (k)')\n",
    "axes[0, 0].set_ylabel('WCSS')\n",
    "axes[0, 0].set_title('Within-Cluster Sum of Squares (Lower is Better)')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# BCSS\n",
    "axes[0, 1].plot(results_df['k'], results_df['bcss'], 'go-', linewidth=2, markersize=8)\n",
    "axes[0, 1].set_xlabel('Number of Clusters (k)')\n",
    "axes[0, 1].set_ylabel('BCSS')\n",
    "axes[0, 1].set_title('Between-Cluster Sum of Squares (Higher is Better)')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Calinski-Harabasz Index\n",
    "axes[1, 0].plot(results_df['k'], results_df['ch_index'], 'mo-', linewidth=2, markersize=8)\n",
    "axes[1, 0].set_xlabel('Number of Clusters (k)')\n",
    "axes[1, 0].set_ylabel('Calinski-Harabasz Index')\n",
    "axes[1, 0].set_title('Calinski-Harabasz Index (Higher is Better)')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Davies-Bouldin Index\n",
    "axes[1, 1].plot(results_df['k'], results_df['db_index'], 'ro-', linewidth=2, markersize=8)\n",
    "axes[1, 1].set_xlabel('Number of Clusters (k)')\n",
    "axes[1, 1].set_ylabel('Davies-Bouldin Index')\n",
    "axes[1, 1].set_title('Davies-Bouldin Index (Lower is Better)')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal k\n",
    "best_k = results_df.loc[results_df['ch_index'].idxmax(), 'k']\n",
    "print(f\"\\nOptimal k by Calinski-Harabasz Index: {best_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Weighted Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with weights\n",
    "weighted_data = spark.createDataFrame([\n",
    "    (Vectors.dense([0.0, 0.0]), 0.1),  # Low weight\n",
    "    (Vectors.dense([0.5, 0.5]), 0.1),\n",
    "    (Vectors.dense([5.0, 5.0]), 10.0),  # High weight\n",
    "    (Vectors.dense([5.5, 5.0]), 10.0),\n",
    "    (Vectors.dense([5.0, 5.5]), 10.0),\n",
    "], [\"features\", \"weight\"])\n",
    "\n",
    "print(\"Weighted data:\")\n",
    "weighted_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster with weights\n",
    "kmeans_weighted = GeneralizedKMeans(\n",
    "    k=2,\n",
    "    weightCol=\"weight\",\n",
    "    maxIter=20,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "model_weighted = kmeans_weighted.fit(weighted_data)\n",
    "\n",
    "print(\"Cluster centers (weighted):\")\n",
    "for i, center in enumerate(model_weighted.clusterCenters()):\n",
    "    print(f\"  Cluster {i}: {center}\")\n",
    "\n",
    "print(\"\\nNote: High-weight points pull cluster centers toward them\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Model Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import shutil\n",
    "from massivedatascience.clusterer import GeneralizedKMeansModel\n",
    "\n",
    "# Save model\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "model_path = f\"{temp_dir}/tutorial_model\"\n",
    "\n",
    "print(f\"Saving model to: {model_path}\")\n",
    "model.write().overwrite().save(model_path)\n",
    "\n",
    "# Load model\n",
    "loaded_model = GeneralizedKMeansModel.load(model_path)\n",
    "print(f\"Loaded model with {loaded_model.numClusters} clusters\")\n",
    "\n",
    "# Verify predictions match\n",
    "original_preds = [row.prediction for row in model.transform(data).collect()]\n",
    "loaded_preds = [row.prediction for row in loaded_model.transform(data).collect()]\n",
    "\n",
    "print(f\"Predictions match: {original_preds == loaded_preds}\")\n",
    "\n",
    "# Clean up\n",
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial covered:\n",
    "1. Basic clustering with Squared Euclidean distance\n",
    "2. Clustering probability distributions with KL divergence\n",
    "3. Using quality metrics to find optimal k\n",
    "4. Weighted clustering for important points\n",
    "5. Model persistence (save/load)\n",
    "\n",
    "### Next Steps\n",
    "- Try other divergences: `itakuraSaito`, `generalizedI`, `logisticLoss`\n",
    "- Experiment with initialization modes: `random`, `k-means||`\n",
    "- Use assignment strategies: `auto`, `broadcast`\n",
    "- Integrate with Spark ML Pipelines\n",
    "- Apply to real-world datasets\n",
    "\n",
    "### Documentation\n",
    "- GitHub: https://github.com/massivedatascience/generalized-kmeans-clustering\n",
    "- Examples: See `python/examples/` directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
