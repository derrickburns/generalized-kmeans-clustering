# Generalized K-Means Clustering

[![CI](https://github.com/derrickburns/generalized-kmeans-clustering/actions/workflows/ci.yml/badge.svg)](https://github.com/derrickburns/generalized-kmeans-clustering/actions/workflows/ci.yml)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Scala 2.13](https://img.shields.io/badge/scala-2.13.14-red.svg)](https://www.scala-lang.org/)
[![Scala 2.12](https://img.shields.io/badge/scala-2.12.18-red.svg)](https://www.scala-lang.org/)
[![Spark 3.5](https://img.shields.io/badge/spark-3.5.1-orange.svg)](https://spark.apache.org/)

üÜï DataFrame API (Spark ML) is the default.
Version 0.6.0 introduces a modern, RDD-free DataFrame-native API with Spark ML integration.
See DataFrame API Examples for end-to-end usage.

This project generalizes K-Means to multiple Bregman divergences and advanced variants (Bisecting, X-Means, Soft/Fuzzy, Streaming, K-Medians, K-Medoids). It provides:
	‚Ä¢	A DataFrame/ML API (recommended), and
	‚Ä¢	A legacy RDD API kept for backwards compatibility (archived below).

What‚Äôs in here
	‚Ä¢	Multiple divergences: Squared Euclidean, KL, Itakura‚ÄìSaito, L1/Manhattan (K-Medians), Generalized-I, Logistic-loss
	‚Ä¢	Variants: Bisecting, X-Means (BIC/AIC), Soft K-Means, Structured-Streaming K-Means, K-Medoids (PAM/CLARA)
	‚Ä¢	Scale: Tested on tens of millions of points in 700+ dimensions
	‚Ä¢	Tooling: Scala 2.13 (primary), Spark 3.5.x (default, with 3.4.x compatibility)

‚∏ª

Quick Start (DataFrame API)

Recommended for all new projects. The DataFrame API follows the Spark ML Estimator/Model pattern.

import org.apache.spark.ml.linalg.Vectors
import com.massivedatascience.clusterer.ml.GeneralizedKMeans

val df = spark.createDataFrame(Seq(
  Tuple1(Vectors.dense(0.0, 0.0)),
  Tuple1(Vectors.dense(1.0, 1.0)),
  Tuple1(Vectors.dense(9.0, 8.0)),
  Tuple1(Vectors.dense(8.0, 9.0))
)).toDF("features")

val gkm = new GeneralizedKMeans()
  .setK(2)
  .setDivergence("kl")              // "squaredEuclidean", "itakuraSaito", "l1", "generalizedI", "logistic"
  .setAssignmentStrategy("auto")    // "auto" | "crossJoin" (SE fast path) | "broadcastUDF" (general Bregman)
  .setMaxIter(20)

val model = gkm.fit(df)
val pred  = model.transform(df)
pred.show(false)

More recipes: see DataFrame API Examples.

‚∏ª

Feature Matrix

Algorithm	DataFrame API	Use Case	Key Benefit
GeneralizedKMeans	‚úÖ	General clustering	5+ Bregman divergences
Bisecting K-Means	‚úÖ	Hierarchical/divisive	Tree structure; no k tuning
X-Means	‚úÖ	Unknown k	Automatic k via BIC/AIC
Soft K-Means	‚úÖ	Fuzzy	Probabilistic memberships
Streaming K-Means	‚úÖ	Real-time	Exponential forgetting
K-Medoids (PAM/CLARA)	‚úÖ	Outlier-robust	Medoids; custom distances
K-Medians	‚úÖ	Robust	L1/Manhattan divergence
Constrained K-Means	‚ö†Ô∏è RDD only	Balance/capacity	Size constraints
Mini-Batch K-Means	‚ö†Ô∏è RDD only	Massive datasets	Sampling-based
Coreset K-Means	‚ö†Ô∏è RDD only	Massive datasets	Approximation/acceleration


‚∏ª

Installation / Versions
	‚Ä¢	Spark: 3.5.1 default (override via -Dspark.version), 3.4.x tested
	‚Ä¢	Scala: 2.13.14 (primary), 2.12.18 (cross-compiled)
	‚Ä¢	Java: 17

libraryDependencies += "com.massivedatascience" %% "massivedatascience-clusterer" % "0.6.0"

What‚Äôs New in 0.6.0
	‚Ä¢	Scala 2.13 primary; 3.5.x Spark default
	‚Ä¢	DataFrame API implementations for: Bisecting, X-Means, Soft, Streaming, K-Medoids
	‚Ä¢	K-Medians (L1) divergence support
	‚Ä¢	PySpark wrapper + smoke test
	‚Ä¢	Expanded examples & docs

‚∏ª

Scaling & Assignment Strategy (important)

Different divergences require different assignment mechanics at scale:
	‚Ä¢	Squared Euclidean (SE) fast path ‚Äî expression/codegen route:
	1.	Cross-join points with centers
	2.	Compute squared distance column
	3.	Prefer groupBy(rowId).min(distance) ‚Üí join to pick argmin (scales better than window sorts)
	4.	Requires a stable rowId; we provide a RowIdProvider.
	‚Ä¢	General Bregman ‚Äî broadcast + UDF route:
	‚Ä¢	Broadcast the centers; compute argmin via a tight JVM UDF.
	‚Ä¢	Broadcast ceiling: you‚Äôll hit executor/memory limits if k √ó dim is too large to broadcast.

Parameters
	‚Ä¢	assignmentStrategy: StringParam = auto | crossJoin | broadcastUDF
	‚Ä¢	auto chooses SE fast path when divergence == SE and feasible; otherwise broadcastUDF.
	‚Ä¢	broadcastThreshold: IntParam (elements, not bytes)
	‚Ä¢	Heuristic ceiling for k √ó dim to guard broadcasts. If exceeded for non-SE, we warn and keep the broadcastUDF path (no DF fallback exists for general Bregman).

‚∏ª

Input Transforms & Interpretation

Some divergences (KL, IS) require positivity or benefit from stabilized domains.
	‚Ä¢	inputTransform: StringParam = none | log1p | epsilonShift
	‚Ä¢	shiftValue: DoubleParam (e.g., 1e-6) when epsilonShift is used.

Note: Cluster centers are learned in the transformed space. If you need original-space interpretation, apply the appropriate inverse (e.g., expm1) for reporting, understanding that this is an interpretive mapping, not a different optimum.

‚∏ª

Bisecting K-Means ‚Äî efficiency note

The driver maintains a cluster_id column. For each split:
	1.	Filter only the target cluster: df.where(col("cluster_id") === id)
	2.	Run the base learner on that subset (k=2)
	3.	Join back predictions to update only the touched rows

This avoids reshuffling the full dataset at every split.

‚∏ª

Structured Streaming K-Means

Estimator/Model for micro-batch streams using the same core update logic.
	‚Ä¢	initStrategy = pretrained | randomFirstBatch
	‚Ä¢	pretrained: provide setInitialModel / setInitialCenters
	‚Ä¢	randomFirstBatch: seed from the first micro-batch
	‚Ä¢	State & snapshots: Each micro-batch writes centers to
${checkpointDir}/centers/latest.parquet for batch reuse.
	‚Ä¢	StreamingGeneralizedKMeansModel.read(path) reconstructs a batch model from snapshots.

‚∏ª

Persistence (Spark ML)

Models implement DefaultParamsWritable/Readable.

Layout

<path>/
  ‚îú‚îÄ metadata/params.json
  ‚îú‚îÄ centers/*.parquet          # (center_id, vector[, weight])
  ‚îî‚îÄ summary/*.json             # events, metrics (optional)

Compatibility
	‚Ä¢	Save/Load verified across Spark 3.4.x ‚Üî 3.5.x in CI.
	‚Ä¢	New params default safely on older loads; unknown params are ignored.

‚∏ª

Python (PySpark) wrapper
	‚Ä¢	Package exposes GeneralizedKMeans, BisectingGeneralizedKMeans, SoftGeneralizedKMeans, StreamingGeneralizedKMeans, KMedoids, etc.
	‚Ä¢	CI runs a spark-submit smoke test on local[*] with a non-SE divergence.

‚∏ª

Legacy RDD API (Archived)

Status: Kept for backward compatibility. New development should use the DataFrame API.
The material below documents the original RDD interfaces and helper objects. Some snippets show API signatures (placeholders) rather than runnable examples.

Quick Start (Legacy RDD API)

import com.massivedatascience.clusterer.KMeans
import org.apache.spark.mllib.linalg.Vectors

val data = sc.parallelize(Array(
  Vectors.dense(0.0, 0.0),
  Vectors.dense(1.0, 1.0),
  Vectors.dense(9.0, 8.0),
  Vectors.dense(8.0, 9.0)
))

val model = KMeans.train(
  data,
  runs = 1,
  k = 2,
  maxIterations = 20
)


‚∏ª

The remainder of this section is an archived reference for the RDD API.

It includes: Bregman divergences, BregmanPoint/BregmanCenter, KMeansModel, clusterers, seeding, embeddings, iterative training, coreset helpers, and helper object builders.
Code blocks that include ??? indicate signatures in the original design.

<details>
<summary>Open archived RDD documentation</summary>


<!-- BEGIN LEGACY CONTENT (unchanged) -->


(All of your original README RDD content goes here ‚Äî exactly as provided in your message.
For brevity in this chat, I‚Äôm not duplicating it again, but in your repo, place the full section here.)

<!-- END LEGACY CONTENT -->


</details>



‚∏ª

Table of Contents
	‚Ä¢	Generalized K-Means Clustering
	‚Ä¢	Quick Start (DataFrame API)
	‚Ä¢	Feature Matrix
	‚Ä¢	Installation / Versions
	‚Ä¢	Scaling & Assignment Strategy
	‚Ä¢	Input Transforms & Interpretation
	‚Ä¢	Bisecting K-Means ‚Äî efficiency note
	‚Ä¢	Structured Streaming K-Means
	‚Ä¢	Persistence (Spark ML)
	‚Ä¢	Python (PySpark) wrapper
	‚Ä¢	Legacy RDD API (Archived)

‚∏ª

Contributing
	‚Ä¢	Please prefer PRs that target the DataFrame/ML path.
	‚Ä¢	Add tests (including property-based where sensible) and update examples.
	‚Ä¢	Follow Conventional Commits (feat:, fix:, docs:, refactor:, test:).

‚∏ª

License

Apache 2.0

‚∏ª

Notes for maintainers (can be removed later)
	‚Ä¢	As you land more DF features, consider extracting the RDD material into LEGACY_RDD.md to keep the README short.
	‚Ä¢	Keep the ‚ÄúScaling & Assignment Strategy‚Äù section up-to-date when adding SE accelerations (Hamerly/Elkan/Yinyang) or ANN-assisted paths‚Äîmark SE-only and exact/approximate as appropriate.
