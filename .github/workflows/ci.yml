name: CI

on:
  push:
    branches: [ master, main ]
  pull_request:
    branches: [ master, main ]
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  DEFAULT_SPARK: "3.5.1"
  PYSPARK_PIN: "3.5.1"
  JAVA_VERSION: "17"
  SCALA_212: "2.12.18"
  SCALA_213: "2.13.14"
  SBT_OPTS: "-Xms2g -Xmx4g -XX:+UseG1GC -Dsbt.log.noformat=true"

jobs:
  # Job 0: Linting and Code Style (fast gate)
  lint:
    runs-on: ubuntu-latest
    name: Lint & Style Check
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: ${{ env.JAVA_VERSION }}
          cache: sbt
      - uses: sbt/setup-sbt@v1
      - name: Check Formatting and Style
        run: |
          sbt ++${{ env.SCALA_213 }} scalafmtCheckAll
          sbt ++${{ env.SCALA_213 }} scalastyle

  # Job 1: Build once (produce 2.12 & 2.13 jars for reuse)
  build:
    runs-on: ubuntu-latest
    needs: lint
    name: Build JARs (2.12 & 2.13)
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: ${{ env.JAVA_VERSION }}
          cache: sbt
      - uses: sbt/setup-sbt@v1
      - name: Package 2.12 (Spark ${{ env.DEFAULT_SPARK }})
        run: sbt ++${{ env.SCALA_212 }} -Dspark.version=${{ env.DEFAULT_SPARK }} clean package
      - name: Package 2.13 (Spark ${{ env.DEFAULT_SPARK }})
        run: sbt ++${{ env.SCALA_213 }} -Dspark.version=${{ env.DEFAULT_SPARK }} clean package
      - name: Upload JARs
        uses: actions/upload-artifact@v4
        with:
          name: jars
          path: |
            target/scala-2.12/*.jar
            target/scala-2.13/*.jar
          if-no-files-found: error

  # Job 2: Core JVM tests across matrix (re-compiles per combo; ok)
  test-jvm:
    runs-on: ubuntu-latest
    needs: build
    strategy:
      fail-fast: false
      matrix:
        scala-version: [ '2.13.14', '2.12.18' ]
        spark-version: [ '3.4.3', '3.5.1' ]
        include:
          # Force Java 17 for Spark 3.5/3.4
          - java-version: '17'
        exclude:
          # Spark 3.4 typically ships with Scala 2.12 artifacts
          - scala-version: '2.13.14'
            spark-version: '3.4.3'
    name: Test (Scala ${{ matrix.scala-version }}, Spark ${{ matrix.spark-version }})
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: ${{ matrix.java-version }}
          cache: sbt
      - uses: sbt/setup-sbt@v1
      - name: Run All JVM Tests
        run: sbt ++${{ matrix.scala-version }} -Dspark.version=${{ matrix.spark-version }} test
      - name: Preserve test reports
        if: always()
        run: |
          mkdir -p artifacts/test-reports/${{ matrix.scala-version }}_${{ matrix.spark-version }}
          (test -d target/test-reports && cp -r target/test-reports/* artifacts/test-reports/${{ matrix.scala-version }}_${{ matrix.spark-version }}/) || true
      - uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-reports-${{ matrix.scala-version }}-${{ matrix.spark-version }}
          path: artifacts/test-reports/${{ matrix.scala-version }}_${{ matrix.spark-version }}

  # Job 3: Python smoke test (downloads jar built once; uses non-SE divergence)
  test-python:
    runs-on: ubuntu-latest
    needs: build
    name: Python Smoke Test (PySpark ${{ env.PYSPARK_PIN }})
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: ${{ env.JAVA_VERSION }}
      - name: Download JARs
        uses: actions/download-artifact@v4
        with:
          name: jars
          path: jars
      - name: Install PySpark
        run: python -m pip install --upgrade pip && pip install pyspark==${{ env.PYSPARK_PIN }}
      - name: Run smoke (local[*], non-SE)
        run: |
          JAR_212=$(ls jars/*scala-2.12*.jar | head -n1)
          test -f "$JAR_212"
          spark-submit --jars "$JAR_212" python/smoke_test.py

  # Job 4: Run examples via runMain with assertions
  examples-run:
    runs-on: ubuntu-latest
    needs: build
    name: Examples (runMain)
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: ${{ env.JAVA_VERSION }}
          cache: sbt
      - uses: sbt/setup-sbt@v1
      - name: Run examples via runMain
        run: |
          sbt ++${{ env.SCALA_213 }} -Dspark.version=${{ env.DEFAULT_SPARK }} "runMain examples.BisectingExample"
          sbt ++${{ env.SCALA_213 }} -Dspark.version=${{ env.DEFAULT_SPARK }} "runMain examples.XMeansExample"
          sbt ++${{ env.SCALA_213 }} -Dspark.version=${{ env.DEFAULT_SPARK }} "runMain examples.SoftKMeansExample"

  # Job 5: Cross-version persistence check (save on 3.4, load on 3.5; and reverse)
  persistence-cross:
    runs-on: ubuntu-latest
    needs: build
    name: Persistence Cross-Version (3.4 ↔ 3.5)
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: ${{ env.JAVA_VERSION }}
          cache: sbt
      - uses: sbt/setup-sbt@v1

      - name: Save with Spark 3.4.x
        run: sbt ++${{ env.SCALA_213 }} -Dspark.version=3.4.3 "runMain examples.PersistenceRoundTrip save ./tmp_model_34"

      - name: Load with Spark 3.5.x
        run: sbt ++${{ env.SCALA_213 }} -Dspark.version=${{ env.DEFAULT_SPARK }} "runMain examples.PersistenceRoundTrip load ./tmp_model_34"

      - name: Save with Spark 3.5.x
        run: sbt ++${{ env.SCALA_213 }} -Dspark.version=${{ env.DEFAULT_SPARK }} "runMain examples.PersistenceRoundTrip save ./tmp_model_35"

      - name: Load with Spark 3.4.x
        run: sbt ++${{ env.SCALA_213 }} -Dspark.version=3.4.3 "runMain examples.PersistenceRoundTrip load ./tmp_model_35"

  # Job 6: Perf sanity (SE and non-SE) — logs perf_sanity_seconds=...
  perf-sanity:
    runs-on: ubuntu-latest
    needs: build
    name: Perf Sanity
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: ${{ env.JAVA_VERSION }}
          cache: sbt
      - uses: sbt/setup-sbt@v1
      - name: Run Perf Sanity (SE & KL)
        run: |
          sbt ++${{ env.SCALA_213 }} -Dspark.version=${{ env.DEFAULT_SPARK }} "testOnly *PerfSanitySuite"
      - name: Surface perf metrics
        run: |
          grep -E 'perf_sanity_seconds=' target/scala-2.13/test-reports/* || true

  # Job 7: Coverage (unchanged, runs once)
  coverage:
    runs-on: ubuntu-latest
    needs: build
    name: Code Coverage
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: ${{ env.JAVA_VERSION }}
          cache: sbt
      - uses: sbt/setup-sbt@v1
      - name: Generate Coverage Report
        run: sbt ++${{ env.SCALA_212 }} -Dspark.version=${{ env.DEFAULT_SPARK }} coverage test coverageReport
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          file: ./target/scala-2.12/scoverage-report/scoverage.xml
          fail_ci_if_error: false

  # Final gate: only reports success if all validations pass
  release-ready:
    runs-on: ubuntu-latest
    needs:
      - lint
      - test-jvm
      - test-python
      - examples-run
      - persistence-cross
      - perf-sanity
      - coverage
    steps:
      - run: echo "All validations passed."
